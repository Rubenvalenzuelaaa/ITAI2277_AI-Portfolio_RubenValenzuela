# Phase 02 â€“ Data Pipeline & Exploratory Data Analysis (EDA)

**Course:** ITAI 2277 â€“ Artificial Intel Resource  
**Project:** AI Course Copilot â€“ RAG-Powered Study & Project Assistant  
**Team Members:** Oscar Cortez, Judith Barrios, Ruben Valenzuela Alvarado, Darnell Newman  

---

## ğŸ“˜ Overview
This phase focused on building the complete **data pipeline** for the RAG-based AI Course Copilot.  
The work includes data collection, preprocessing, chunking, embedding generation, dataset organization, and exploratory data analysis.

The goal was to create a clean, well-structured dataset suitable for semantic retrieval and later integration in Phase 03 (Working AI Model).

---

## ğŸ—‚ï¸ Phase Deliverables
### âœ” 1. Data Report (Uploaded as DOCX)
A detailed written report describing:

- Data sources  
- Collection strategy  
- Cleaning and preprocessing steps  
- Feature engineering  
- Chunking and embedding pipeline  
- Exploratory visualizations  
- Insights and observations  

ğŸ‘‰ **File:** `Phase02_Data_Pipeline_and_EDA.docx`

---

## ğŸ” Week 4 â€“ Data Collection & Organization

### **Data Sources**
We collected only instructor-approved and publicly available materials:
- Course syllabi  
- Assignment instructions  
- Rubrics  
- Project guidelines  
- Public AI/ML references  

### **Collection Methods**
- Manual review of course files  
- Automated extraction using:
  - **PyPDF2** (PDF parsing)
  - **BeautifulSoup / Requests** (web scraping)
- Metadata assigned to each file (course, week, topic)

### **Dataset Structure**   
data/
â”œâ”€â”€ raw/ # Original documents
â”œâ”€â”€ processed/ # Clean & normalized text
â”œâ”€â”€ embeddings/ # Vector embeddings
â””â”€â”€ metadata.json # Metadata for indexing

We follow strict ethical guidelines:
- No personal student data  
- No copyrighted material  
- Only allow-listed files  

---

## ğŸ”§ Week 5 â€“ Preprocessing & Feature Engineering

### **Preprocessing Steps**
- Remove headers, footers, noise  
- Normalize whitespace  
- Fix encoding  
- Convert text to UTF-8  
- Remove duplicates  

### **Chunking Strategy**
- 300â€“500 token segments  
- Each chunk receives metadata:
  - Course name  
  - Topic  
  - Document type  
  - Week number  

### **Embedding Generation**
Models used:
- **SentenceTransformers**  
- **OpenAI text-embedding models**  

Pipeline:
clean_text â†’ chunk â†’ embed â†’ store_in_vectorDB

### **Dataset Split**
Created:
- Training set  
- Validation set  
- Test set  

To benchmark retrieval performance before Phase 03.

---

## ğŸ“Š Week 6 â€“ Exploratory Data Analysis (EDA)

### **Statistical Findings**
- Average document size  
- Distribution of chunk lengths  
- Keyword frequency (`AI`, `ethics`, `retrieval`, etc.)  

### **Visualizations**
Generated using Matplotlib:
- Histogram of document tokens  
- Word cloud  
- Bar chart of document counts per week  

### **Insights**
- Some rubric and instruction files contain overlapping text â†’ cleaned via semantic deduplication  
- Ethics/policy documents are high-value data for accurate retrieval  
- Balanced indexing improves RAG answer quality  

---

## ğŸ§© Phase Insights
- Clean, consistent text drastically improves embedding quality  
- Organized metadata structure ensures reliable vector search  
- EDA revealed imbalances across course weeks â†’ corrected in indexing  
- Prepared all assets for Phase 03 model training and validation  

---

## ğŸ“¦ Included in This Folder
- **Phase02_Report.docx**  
- `README.md` (this file)

---

## ğŸ”— Next Steps
Phase 03 will incorporate:
- Chunked & embedded dataset  
- Retrieval benchmarking (BM25 vs RAG)  
- Full working model for the AI Course Copilot  

---
